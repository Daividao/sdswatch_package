input {
  file {
    path => ["/jobs/**/**/**/**/**/**/*.sdswatch.log"]
    start_position => "beginning" # read the log files from the beginning
    type => "pge" # add "type" field with value "pge"
  }
  file {
    path => ["/verdi/*.sdswatch.log"]
    start_position => "beginning"
    type => "worker" # add "worker" field with value "worker"
  }
}

# when migrating Logstash to Filebeat,
# you want to make sure the log coming to have the type field with either "pge" or "worker" value. 


filter {
  if [type] == "worker" { # use [field] to access value field
    csv {
      source => "message" # parse csv format in [message]
      separator => "," # split by comma
      columns => ["sdswatch_timestamp", "host", "source_type", "source_id", "metric_key", "metric_value"]
      quote_char => "'" # allow comma in value
    } 
    mutate {
      strip => ["sdswatch_timestamp", "host", "source_type", "source_id", "metric_key", "metric_value"]
    }
  } else if [type] == "pge" {
    csv {
      source => "message"
      separator => ","
      columns => ["sdswatch_timestamp", "metric_key", "metric_value"]
      quote_char => "'"
    }
    mutate {
      remove_field => ["host"]
      strip => ["sdswatch_timestamp", "metric_key", "metric_value"]
    }
    ruby { # use ruby to handle tasks that Logstash doesn't support
           # in this case, I'm trying to extract <source_id> and <source_type> from file path
      code => '
        path = event.get("path").split("/")
        source_type = path[8].delete_suffix(".pge.sdswatch.log")
        source_id = path[7]
        event.set("source_type", source_type)
        event.set("source_id", source_id)                       
      '
    }
    mutate { # ${HOST} is used to access environment variable
             # you will also want to do similarly with Filebeat client
             # because we need the "host" from job_worker to field in missing "host" of pge sdswatch logs
             # In addition, when migrating to Filebeat, it's a little bit tricky here.
             # Since you need to move the filtering part to SDSWatch server,
             # you cannot do ${HOST} anymore. I guess you have to add another field to Filebeat output on the client side.
             # I suggest adding "source_host" field from the client side, shouldn't  use "host" because there is a default "host"
             # field
       add_field => {"host" => "${HOST}"}
    }    
  }
  
  mutate { # add field "log_path" with value in "path", we use %{path} instead of [path] because it is inside a double quote
    add_field => { "log_path" => "%{path}" }
    remove_field => ["@timestamp", "path", "type"]
  }

  date {
    match => ["sdswatch_timestamp", "yyyy-MM-dd HH:mm:ss.SSS", "yyyy-MM-dd HH:mm:ss,SSS", "yyyy-MM-dd'T'HH:mm:ssZ" ] 
    timezone => "UTC"
    target => "sdswatch_timestamp"
  }

  ruby {
    code => '
      key = event.get("metric_key")
      value = event.get("metric_value")
      numeric_value = -1
      if value.to_f.to_s == value || value.to_i.to_s == value
         numeric_value = value.to_f
      end
      event.set("metric_value_float", numeric_value)
    '
  }

  mutate {
    rename => ["metric_value", "metric_value_string"]
  }
}


output { # send data to SDSWatch server
  redis {
    host => "100.67.35.12:6379"
    data_type => "list"
    key => "logstash"
  }
  stdout {
    codec => rubydebug
  }
}
